llm:
  provider: OLLAMA

  meta:
    model: llama3
    max_tokens: 4096
    temperature: 0.2

  http_client:
    timeout: 120
    api_url: http://localhost:11434

vcs:
  provider: GITHUB

  http_client:
    timeout: 120
    api_url: https://api.github.com

prompt:
  inline_prompt_files: [ ./prompts/inline.md ]
  context_prompt_files: [ ./prompts/inline.md ]
  summary_prompt_files: [ ./prompts/summary.md ]

review:
  allow_changes: [ src/** ]