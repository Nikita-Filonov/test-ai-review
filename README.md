# AI Review with Local LLM (Ollama) â€” Demo Project

This repository demonstrates how to integrate [AI Review](https://github.com/Nikita-Filonov/ai-review) â€” an open-source
automated code review tool â€” with a local LLM powered by [Ollama](https://ollama.com/).

It shows how to run fully local AI code
reviews [directly inside your CI/CD pipeline](https://github.com/Nikita-Filonov/test-ai-review/actions) (e.g. GitHub
Actions) â€”

- âœ… without API tokens,
- âœ… without sending code to external services,
- âœ… and completely for free.

The demo includes:

- ğŸ”§ Minimal [AI Review](https://github.com/Nikita-Filonov/ai-review) configuration ([.ai-review.yaml](.ai-review.yaml))
- âš™ï¸ [GitHub Actions workflow](https://github.com/Nikita-Filonov/test-ai-review/actions/runs/18259278251) with a local
  Ollama service
- ğŸ¤– Example review
  results ([inline comments](https://github.com/Nikita-Filonov/test-ai-review/pull/2#discussion_r2404479873)
    + [summary](https://github.com/Nikita-Filonov/test-ai-review/pull/2#issuecomment-3369053823))
- ğŸ’¡ This setup works entirely offline â€” your code never leaves the repository, and the review is generated by a local
  LLM model (e.g. mistral, llama3, or phi3).

If you want to learn more about advanced configuration, custom prompts, or integration with other CI systems (GitLab,
Jenkins, Bitbucket), see the [AI Review documentation](https://github.com/Nikita-Filonov/ai-review/tree/main/docs).