# AI Review with Local LLM (Ollama) — Demo Project

This repository demonstrates how to integrate [AI Review](https://github.com/Nikita-Filonov/ai-review) — an open-source
automated code review tool — with a local LLM powered by [Ollama](https://ollama.com/).

It shows how to run fully local AI code
reviews [directly inside your CI/CD pipeline](https://github.com/Nikita-Filonov/test-ai-review/actions) (e.g. GitHub
Actions) —

- ✅ without API tokens,
- ✅ without sending code to external services,
- ✅ and completely for free.

The demo includes:

- 🔧 Minimal [AI Review](https://github.com/Nikita-Filonov/ai-review) configuration ([.ai-review.yaml](.ai-review.yaml))
- ⚙️ [GitHub Actions workflow](https://github.com/Nikita-Filonov/test-ai-review/actions/runs/18259278251) with a local
  Ollama service
- 🤖 Example review
  results ([inline comments](https://github.com/Nikita-Filonov/test-ai-review/pull/2#discussion_r2404479873)
    + [summary](https://github.com/Nikita-Filonov/test-ai-review/pull/2#issuecomment-3369053823))
- 💡 This setup works entirely offline — your code never leaves the repository, and the review is generated by a local
  LLM model (e.g. mistral, llama3, or phi3).

If you want to learn more about advanced configuration, custom prompts, or integration with other CI systems (GitLab,
Jenkins, Bitbucket), see the [AI Review documentation](https://github.com/Nikita-Filonov/ai-review/tree/main/docs).